{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986849d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0e1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6f0906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# --> 1 counts\n",
      "Apache --> 1 counts\n",
      "Spark --> 15 counts\n",
      " --> 72 counts\n",
      "is --> 7 counts\n",
      "a --> 9 counts\n",
      "fast --> 1 counts\n",
      "and --> 10 counts\n",
      "general --> 3 counts\n",
      "cluster --> 2 counts\n"
     ]
    }
   ],
   "source": [
    "file_path = 'file:////home/talentum/spark/README.md'\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    " \n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_map = splitRDD.map(lambda w: (w.strip(), 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_map.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(\"{} --> {} counts\". format(word[0], word[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce2f32",
   "metadata": {},
   "source": [
    "Running this file in terminal \n",
    "\n",
    "\n",
    "```python\n",
    "#!/usr/bin/python\n",
    "\n",
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "file_path = 'file:////home/talentum/spark/README.md'\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    " \n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_map = splitRDD.map(lambda w: (w.strip(), 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_map.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(\"{} --> {} counts\". format(word[0], word[1]))\n",
    "```\n",
    "\n",
    "\n",
    "```shell\n",
    "talentum@talentum-virtual-machine:~$ which python\n",
    "/usr/bin/python\n",
    "talentum@talentum-virtual-machine:~$ gedit wordcount.py\n",
    "talentum@talentum-virtual-machine:~$ spark-submit\n",
    "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
    "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
    "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
    "Usage: spark-submit run-example [options] example-class [example args]\n",
    "\n",
    "Options:\n",
    "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
    "                              k8s://https://host:port, or local (Default: local[*]).\n",
    "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
    "                              on one of the worker machines inside the cluster (\"cluster\")\n",
    "                              (Default: client).\n",
    "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
    "  --name NAME                 A name of your application.\n",
    "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
    "                              and executor classpaths.\n",
    "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
    "                              on the driver and executor classpaths. Will search the local\n",
    "                              maven repo, then maven central and any additional remote\n",
    "                              repositories given by --repositories. The format for the\n",
    "                              coordinates should be groupId:artifactId:version.\n",
    "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
    "                              resolving the dependencies provided in --packages to avoid\n",
    "                              dependency conflicts.\n",
    "  --repositories              Comma-separated list of additional remote repositories to\n",
    "                              search for the maven coordinates given with --packages.\n",
    "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
    "                              on the PYTHONPATH for Python apps.\n",
    "  --files FILES               Comma-separated list of files to be placed in the working\n",
    "                              directory of each executor. File paths of these files\n",
    "                              in executors can be accessed via SparkFiles.get(fileName).\n",
    "\n",
    "  --conf PROP=VALUE           Arbitrary Spark configuration property.\n",
    "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
    "                              specified, this will look for conf/spark-defaults.conf.\n",
    "\n",
    "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
    "  --driver-java-options       Extra Java options to pass to the driver.\n",
    "  --driver-library-path       Extra library path entries to pass to the driver.\n",
    "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
    "                              jars added with --jars are automatically included in the\n",
    "                              classpath.\n",
    "\n",
    "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
    "\n",
    "  --proxy-user NAME           User to impersonate when submitting the application.\n",
    "                              This argument does not work with --principal / --keytab.\n",
    "\n",
    "  --help, -h                  Show this help message and exit.\n",
    "  --verbose, -v               Print additional debug output.\n",
    "  --version,                  Print the version of current Spark.\n",
    "\n",
    " Cluster deploy mode only:\n",
    "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
    "                              (Default: 1).\n",
    "\n",
    " Spark standalone or Mesos with cluster deploy mode only:\n",
    "  --supervise                 If given, restarts the driver on failure.\n",
    "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
    "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
    "\n",
    " Spark standalone and Mesos only:\n",
    "  --total-executor-cores NUM  Total cores for all executors.\n",
    "\n",
    " Spark standalone and YARN only:\n",
    "  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n",
    "                              or all available cores on the worker in standalone mode)\n",
    "\n",
    " YARN-only:\n",
    "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
    "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
    "                              If dynamic allocation is enabled, the initial number of\n",
    "                              executors will be at least NUM.\n",
    "  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n",
    "                              working directory of each executor.\n",
    "  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n",
    "                              secure HDFS.\n",
    "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
    "                              principal specified above. This keytab will be copied to\n",
    "                              the node running the Application Master via the Secure\n",
    "                              Distributed Cache, for renewing the login tickets and the\n",
    "                              delegation tokens periodically.\n",
    "      \n",
    "talentum@talentum-virtual-machine:~$ cat unset_jupyter.sh \n",
    "#!/bin/bash\n",
    "\n",
    "unset PYSPARK_DRIVER_PYTHON\n",
    "unset PYSPARK_DRIVER_PYTHON_OPTS\n",
    "talentum@talentum-virtual-machine:~$ source unset_jupyter.sh \n",
    "talentum@talentum-virtual-machine:~$ spark-submit wordcount.py  \n",
    "25/01/03 14:52:00 WARN util.Utils: Your hostname, talentum-virtual-machine resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s17)\n",
    "25/01/03 14:52:00 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "25/01/03 14:52:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/talentum/wordcount.py\", line 5, in <module>\n",
    "    baseRDD = sc.textFile(file_path)\n",
    "NameError: name 'sc' is not defined\n",
    "25/01/03 14:52:01 INFO util.ShutdownHookManager: Shutdown hook called\n",
    "25/01/03 14:52:01 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-acccd12f-7d91-4298-981d-1f764b255e17\n",
    "talentum@talentum-virtual-machine:~$ gedit wordcount.py\n",
    "talentum@talentum-virtual-machine:~$ spark-submit wordcount.py  \n",
    "25/01/03 14:54:38 WARN util.Utils: Your hostname, talentum-virtual-machine resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s17)\n",
    "25/01/03 14:54:38 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "25/01/03 14:54:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "25/01/03 14:54:40 INFO spark.SparkContext: Running Spark version 2.4.5\n",
    "25/01/03 14:54:40 INFO spark.SparkContext: Submitted application: Spark SQL basic example\n",
    "25/01/03 14:54:40 INFO spark.SecurityManager: Changing view acls to: talentum\n",
    "25/01/03 14:54:40 INFO spark.SecurityManager: Changing modify acls to: talentum\n",
    "25/01/03 14:54:40 INFO spark.SecurityManager: Changing view acls groups to: \n",
    "25/01/03 14:54:40 INFO spark.SecurityManager: Changing modify acls groups to: \n",
    "25/01/03 14:54:40 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(talentum); groups with view permissions: Set(); users  with modify permissions: Set(talentum); groups with modify permissions: Set()\n",
    "25/01/03 14:54:40 INFO util.Utils: Successfully started service 'sparkDriver' on port 41245.\n",
    "25/01/03 14:54:40 INFO spark.SparkEnv: Registering MapOutputTracker\n",
    "25/01/03 14:54:40 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
    "25/01/03 14:54:40 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
    "25/01/03 14:54:40 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
    "25/01/03 14:54:40 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-53f3af7e-d2da-4c09-b712-d11d648b39b0\n",
    "25/01/03 14:54:40 INFO memory.MemoryStore: MemoryStore started with capacity 413.9 MB\n",
    "25/01/03 14:54:40 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
    "25/01/03 14:54:41 INFO util.log: Logging initialized @3213ms\n",
    "25/01/03 14:54:41 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
    "25/01/03 14:54:41 INFO server.Server: Started @3409ms\n",
    "25/01/03 14:54:41 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "25/01/03 14:54:41 INFO server.AbstractConnector: Started ServerConnector@28620b33{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
    "25/01/03 14:54:41 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b773671{/jobs,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@433385de{/jobs/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27cc15d3{/jobs/job,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37db06a6{/jobs/job/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@689978f6{/stages,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f3c8abb{/stages/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fdd2619{/stages/stage,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288342ca{/stages/stage/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e28442d{/stages/pool,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@490835e1{/stages/pool/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21eb6ada{/storage,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@134cf93f{/storage/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f69ce57{/storage/rdd,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37c64366{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d8152fe{/environment,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39f0df70{/environment/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@481aa56f{/executors,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d420279{/executors/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a13af8d{/executors/threadDump,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3527eae0{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@288b10f6{/static,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67a6fd2c{/,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@543277c{/api,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a43779f{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79d7d614{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:41 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4041\n",
    "25/01/03 14:54:41 INFO executor.Executor: Starting executor ID driver on host localhost\n",
    "25/01/03 14:54:41 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37963.\n",
    "25/01/03 14:54:41 INFO netty.NettyBlockTransferService: Server created on 10.0.2.15:37963\n",
    "25/01/03 14:54:41 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
    "25/01/03 14:54:41 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 37963, None)\n",
    "25/01/03 14:54:41 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:37963 with 413.9 MB RAM, BlockManagerId(driver, 10.0.2.15, 37963, None)\n",
    "25/01/03 14:54:41 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 37963, None)\n",
    "25/01/03 14:54:41 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 37963, None)\n",
    "25/01/03 14:54:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ac69269{/metrics/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:42 INFO internal.SharedState: loading hive config file: file:/home/talentum/spark/conf/hive-site.xml\n",
    "25/01/03 14:54:42 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').\n",
    "25/01/03 14:54:42 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.\n",
    "25/01/03 14:54:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67ee1d2d{/SQL,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53cea9a9{/SQL/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a3211eb{/SQL/execution,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cd733bf{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@234e3110{/static/sql,null,AVAILABLE,@Spark}\n",
    "25/01/03 14:54:43 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
    "25/01/03 14:54:44 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 243.2 KB, free 413.7 MB)\n",
    "25/01/03 14:54:44 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.6 KB, free 413.7 MB)\n",
    "25/01/03 14:54:44 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:37963 (size: 23.6 KB, free: 413.9 MB)\n",
    "25/01/03 14:54:44 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
    "25/01/03 14:54:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "25/01/03 14:54:45 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
    "25/01/03 14:54:45 INFO scheduler.DAGScheduler: Registering RDD 3 (reduceByKey at /home/talentum/wordcount.py:25) as input to shuffle 0\n",
    "25/01/03 14:54:45 INFO scheduler.DAGScheduler: Got job 0 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
    "25/01/03 14:54:45 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:153)\n",
    "25/01/03 14:54:45 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
    "25/01/03 14:54:45 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
    "25/01/03 14:54:45 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/talentum/wordcount.py:25), which has no missing parents\n",
    "25/01/03 14:54:45 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.2 KB, free 413.7 MB)\n",
    "25/01/03 14:54:45 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KB, free 413.6 MB)\n",
    "25/01/03 14:54:45 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:37963 (size: 7.3 KB, free: 413.9 MB)\n",
    "25/01/03 14:54:45 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\n",
    "25/01/03 14:54:45 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/talentum/wordcount.py:25) (first 15 tasks are for partitions Vector(0))\n",
    "25/01/03 14:54:45 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
    "25/01/03 14:54:45 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7884 bytes)\n",
    "25/01/03 14:54:45 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
    "25/01/03 14:54:45 INFO rdd.HadoopRDD: Input split: file:/home/talentum/spark/README.md:0+3756\n",
    "25/01/03 14:54:46 INFO python.PythonRunner: Times: total = 437, boot = 414, init = 19, finish = 4\n",
    "25/01/03 14:54:46 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1758 bytes result sent to driver\n",
    "25/01/03 14:54:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1020 ms on localhost (executor driver) (1/1)\n",
    "25/01/03 14:54:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
    "25/01/03 14:54:46 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56841\n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/talentum/wordcount.py:25) finished in 1.317 s\n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: running: Set()\n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: failed: Set()\n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\n",
    "25/01/03 14:54:46 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.9 KB, free 413.6 MB)\n",
    "25/01/03 14:54:46 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KB, free 413.6 MB)\n",
    "25/01/03 14:54:46 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:37963 (size: 5.1 KB, free: 413.9 MB)\n",
    "25/01/03 14:54:46 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163\n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
    "25/01/03 14:54:46 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
    "25/01/03 14:54:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7662 bytes)\n",
    "25/01/03 14:54:46 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
    "25/01/03 14:54:46 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
    "25/01/03 14:54:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
    "25/01/03 14:54:46 INFO python.PythonRunner: Times: total = 36, boot = -643, init = 678, finish = 1\n",
    "25/01/03 14:54:46 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1930 bytes result sent to driver\n",
    "25/01/03 14:54:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 136 ms on localhost (executor driver) (1/1)\n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:153) finished in 0.191 s\n",
    "25/01/03 14:54:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
    "25/01/03 14:54:46 INFO scheduler.DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:153, took 1.786293 s\n",
    "# --> 1 counts\n",
    "Apache --> 1 counts\n",
    "Spark --> 15 counts\n",
    " --> 72 counts\n",
    "is --> 7 counts\n",
    "a --> 9 counts\n",
    "fast --> 1 counts\n",
    "and --> 10 counts\n",
    "general --> 3 counts\n",
    "cluster --> 2 counts\n",
    "25/01/03 14:54:46 INFO spark.SparkContext: Invoking stop() from shutdown hook\n",
    "25/01/03 14:54:46 INFO server.AbstractConnector: Stopped Spark@28620b33{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
    "25/01/03 14:54:46 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.2.15:4041\n",
    "25/01/03 14:54:46 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "25/01/03 14:54:46 INFO memory.MemoryStore: MemoryStore cleared\n",
    "25/01/03 14:54:46 INFO storage.BlockManager: BlockManager stopped\n",
    "25/01/03 14:54:47 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
    "25/01/03 14:54:47 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
    "25/01/03 14:54:47 INFO spark.SparkContext: Successfully stopped SparkContext\n",
    "25/01/03 14:54:47 INFO util.ShutdownHookManager: Shutdown hook called\n",
    "25/01/03 14:54:47 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8bdaaade-d68d-4b50-80e3-f25a5e9bca24/pyspark-7ff7281c-2513-4d18-acfd-755c3191caa3\n",
    "25/01/03 14:54:47 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d9898e92-5160-46f8-9c21-196b24c2b74f\n",
    "25/01/03 14:54:47 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8bdaaade-d68d-4b50-80e3-f25a5e9bca24\n",
    "\n",
    "25/01/03 14:54:46 INFO server.AbstractConnector: Stopped Spark@28620b33{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
    "25/01/03 14:54:46 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.2.15:4041\n",
    "25/01/03 14:54:46 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "25/01/03 14:54:46 INFO memory.MemoryStore: MemoryStore cleared\n",
    "25/01/03 14:54:46 INFO storage.BlockManager: BlockManager stopped\n",
    "25/01/03 14:54:47 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
    "25/01/03 14:54:47 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
    "25/01/03 14:54:47 INFO spark.SparkContext: Successfully stopped SparkContext\n",
    "25/01/03 14:54:47 INFO util.ShutdownHookManager: Shutdown hook called\n",
    "25/01/03 14:54:47 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8bdaaade-d68d-4b50-80e3-f25a5e9bca24/pyspark-7ff7281c-2513-4d18-acfd-755c3191caa3\n",
    "25/01/03 14:54:47 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d9898e92-5160-46f8-9c21-196b24c2b74f\n",
    "25/01/03 14:54:47 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8bdaaade-d68d-4b50-80e3-f25a5e9bca24\n",
    "talentum@talentum-virtual-machine:~$ cd conf\n",
    "bash: cd: conf: No such file or directory\n",
    "talentum@talentum-virtual-machine:~$ cd spark/conf/\n",
    "talentum@talentum-virtual-machine:~/spark/conf$ ls -lh\n",
    "total 48K\n",
    "-rw-r--r-- 1 talentum talentum  996 Feb  3  2020 docker.properties.template\n",
    "-rw-r--r-- 1 talentum talentum 1.1K Feb  3  2020 fairscheduler.xml.template\n",
    "lrwxrwxrwx 1 talentum talentum   38 Oct 29  2021 hive-site.xml -> /home/talentum/hive/conf/hive-site.xml\n",
    "-rw-r--r-- 1 talentum talentum 2.0K Feb  3  2020 log4j.properties.template\n",
    "-rw-r--r-- 1 talentum talentum 7.7K Feb  3  2020 metrics.properties.template\n",
    "-rw-r--r-- 1 talentum talentum  865 Feb  3  2020 slaves.template\n",
    "-rw-r--r-- 1 talentum talentum 1.3K Nov  4  2020 spark-defaults.conf\n",
    "-rw-r--r-- 1 talentum talentum 1.3K Feb  3  2020 spark-defaults.conf.template\n",
    "-rwxr-xr-x 1 talentum talentum 4.2K Nov  4  2020 spark-env.sh\n",
    "-rwxr-xr-x 1 talentum talentum 4.2K Feb  3  2020 spark-env.sh.template\n",
    "talentum@talentum-virtual-machine:~/spark/conf$ cp log4j.properties.template log4j.properties \n",
    "talentum@talentum-virtual-machine:~/spark/conf$ gedit log4j.properties    # change level to WARN\n",
    "talentum@talentum-virtual-machine:~/spark/conf$ cd ../../\n",
    "talentum@talentum-virtual-machine:~$ spark-submit wordcount.py  \n",
    "25/01/03 14:59:02 WARN Utils: Your hostname, talentum-virtual-machine resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s17)\n",
    "25/01/03 14:59:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "25/01/03 14:59:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "25/01/03 14:59:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "# --> 1 counts\n",
    "Apache --> 1 counts\n",
    "Spark --> 15 counts\n",
    " --> 72 counts\n",
    "is --> 7 counts\n",
    "a --> 9 counts\n",
    "fast --> 1 counts\n",
    "and --> 10 counts\n",
    "general --> 3 counts\n",
    "cluster --> 2 counts \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
