{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a2aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b572cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f6c08",
   "metadata": {},
   "source": [
    "$$\\Large\\text{Code}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67fb40",
   "metadata": {},
   "source": [
    "**PairRDD**\n",
    "- Related methods\n",
    "    - `reduceByKeys(func)`\n",
    "    - `groupByKey()`\n",
    "    - `sortByKey()`\n",
    "    - `joinByKey()`\n",
    "- Some more actions\n",
    "    - reduce()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b60dcf5",
   "metadata": {},
   "source": [
    "$$\\text{Creation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5062d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sam', 23), ('Mary', 34), ('Peter', 25)]\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "paraRDD_tuple = sc.parallelize(my_tuple)\n",
    "print(paraRDD_tuple.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "791dacd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sam 23', 'Mary 34', 'Peter 25']\n",
      "[('Sam', 23), ('Mary', 34), ('Peter', 25)]\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "my_list = ['Sam 23', 'Mary 34','Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], int(s.split(' ')[1])))\n",
    "print(regularRDD.collect())\n",
    "print(pairRDD_RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1685a3",
   "metadata": {},
   "source": [
    "$$\\text{reduceByKeys(func)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aee8ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sam', 23), ('Mary', 36), ('Peter', 25)]\n"
     ]
    }
   ],
   "source": [
    "# List of tuples\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25), ('Mary', 2)]\n",
    "\n",
    "# Create an RDD from the list\n",
    "paraRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "# Reduce by key using reduceByKey\n",
    "reduced_paraRDD_tuple = paraRDD_tuple.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect and print the results\n",
    "print(reduced_paraRDD_tuple.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46563993",
   "metadata": {},
   "source": [
    "$$\\text{groupByKeys()}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d25cd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US ['JFK', 'SFO']\n",
      "UK ['LHR']\n",
      "FR ['CGD']\n",
      "<class 'list'>\n",
      "<class 'tuple'>\n",
      "<class 'str'>\n",
      "<class 'pyspark.resultiterable.ResultIterable'>\n",
      "<class 'int'>\n",
      "3\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# List of airports\n",
    "airports = [('US', 'JFK'), ('UK', 'LHR'), ('FR', 'CGD'), ('US', 'SFO')]\n",
    "\n",
    "# Create an RDD from the list\n",
    "regularRDD = sc.parallelize(airports)\n",
    "\n",
    "# Group by key\n",
    "pairRDD_group = regularRDD.groupByKey()\n",
    "\n",
    "# Collect the results\n",
    "pairRDD_group_collected = pairRDD_group.collect()\n",
    "\n",
    "# Print the grouped results\n",
    "for cont, air in pairRDD_group_collected:\n",
    "    print(cont, list(air))\n",
    "\n",
    "# Print the types\n",
    "print(type(pairRDD_group_collected))\n",
    "print(type(pairRDD_group_collected[0]))\n",
    "print(type(pairRDD_group_collected[0][0]))\n",
    "print(type(pairRDD_group_collected[0][1]))\n",
    "\n",
    "# Print the count of the RDD\n",
    "print(type(pairRDD_group.count()))\n",
    "print(pairRDD_group.count())\n",
    "\n",
    "# Print the type of take of the RDD\n",
    "print(type(pairRDD_group.take(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ef065",
   "metadata": {},
   "source": [
    "$$\\text{sortByKeys()}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e33b6f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 30), ('Mary', 34), ('Peter', 25), ('Sam', 23)]\n",
      "[('Sam', 23), ('Peter', 25), ('Mary', 34), ('John', 30)]\n"
     ]
    }
   ],
   "source": [
    "# List of tuples with key-value pairs\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25), ('John', 30)]\n",
    "\n",
    "# Create an RDD from the list\n",
    "paraRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "# Sort the RDD by keys (ascending order by default)\n",
    "sorted_RDD = paraRDD_tuple.sortByKey()\n",
    "\n",
    "# Collect and print the results\n",
    "print(sorted_RDD.collect())\n",
    "sorted_desc_RDD = paraRDD_tuple.sortByKey(ascending=False)\n",
    "\n",
    "# Collect and print the results\n",
    "print(sorted_desc_RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd047a0",
   "metadata": {},
   "source": [
    "$$\\text{joinByKeys()}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b9fbcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mary', (34, 'Doctor')), ('Sam', (23, 'Engineer'))]\n",
      "[('Mary', (34, 'Doctor')), ('Sam', (23, 'Engineer')), ('Peter', (25, None))]\n",
      "[('Mary', (34, 'Doctor')), ('Sam', (23, 'Engineer')), ('John', (None, 'Lawyer'))]\n"
     ]
    }
   ],
   "source": [
    "# First RDD with key-value pairs\n",
    "rdd1 = sc.parallelize([('Sam', 23), ('Mary', 34), ('Peter', 25)])\n",
    "\n",
    "# Second RDD with key-value pairs\n",
    "rdd2 = sc.parallelize([('Sam', 'Engineer'), ('Mary', 'Doctor'), ('John', 'Lawyer')])\n",
    "\n",
    "# Perform an inner join on the RDDs\n",
    "joined_RDD = rdd1.join(rdd2)\n",
    "\n",
    "# Collect and print the results\n",
    "print(joined_RDD.collect())\n",
    " \n",
    "# Perform a left outer join\n",
    "left_joined_RDD = rdd1.leftOuterJoin(rdd2)\n",
    "# Collect and print the results\n",
    "print(left_joined_RDD.collect())\n",
    "\n",
    "# Perform a right outer join\n",
    "right_joined_RDD = rdd1.rightOuterJoin(rdd2)\n",
    "# Collect and print the results\n",
    "print(right_joined_RDD.collect())\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
