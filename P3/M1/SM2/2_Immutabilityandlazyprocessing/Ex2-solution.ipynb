{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b65821",
   "metadata": {},
   "source": [
    "# Using lazy processing\n",
    "\n",
    "- Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "- For this exercise, we'll be defining a Data Frame (`aa_dfw_df`) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50ef46",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Load the Data Frame.\n",
    "- Add the transformation for `F.lower()` to the `Destination Airport` column.\n",
    "- Drop the `Destination Airport` column from the Data Frame `aa_dfw_df`. Note the time for these operations to complete.\n",
    "- Show the Data Frame, noting the time difference for this action to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a571d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/talentum/test-jupyter/P3/M1/SM2/2_Immutabilityandlazyprocessing\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0ec361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2018|         0005|                          498|    hnl|\n",
      "|       01/01/2018|         0007|                          501|    ogg|\n",
      "|       01/01/2018|         0043|                            0|    dtw|\n",
      "|       01/01/2018|         0051|                          100|    stl|\n",
      "|       01/01/2018|         0075|                          147|    dca|\n",
      "|       01/01/2018|         0096|                           92|    stl|\n",
      "|       01/01/2018|         0103|                          227|    sjc|\n",
      "|       01/01/2018|         0119|                          517|    ogg|\n",
      "|       01/01/2018|         0123|                          489|    hnl|\n",
      "|       01/01/2018|         0128|                          141|    mco|\n",
      "|       01/01/2018|         0132|                          201|    ewr|\n",
      "|       01/01/2018|         0140|                          215|    sjc|\n",
      "|       01/01/2018|         0174|                          140|    rdu|\n",
      "|       01/01/2018|         0190|                           68|    sat|\n",
      "|       01/01/2018|         0200|                          215|    sfo|\n",
      "|       01/01/2018|         0209|                          169|    mia|\n",
      "|       01/01/2018|         0217|                          178|    las|\n",
      "|       01/01/2018|         0229|                          534|    koa|\n",
      "|       01/01/2018|         0244|                          115|    cvg|\n",
      "|       01/01/2018|         0262|                          159|    mia|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('file:///home/talentum/test-jupyter/P3/M1/SM2/2_Immutabilityandlazyprocessing/Dataset/AA_DFW_2018_Departures_Short.csv.gz')\n",
    "print(aa_dfw_df.printSchema())\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df1 = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "print(aa_dfw_df.printSchema())   # Dataframe schema is still the same  - immutable\n",
    "print(aa_dfw_df1.printSchema())  # New Dataframe schema \n",
    "\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df1 = aa_dfw_df1.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b81c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "file_path = 'file:///home/talentum/test-jupyter/P3/M1/SM2/2_Immutabilityandlazyprocessing/Dataset/AA_DFW_2018_Departures_Short.csv.gz'\n",
    "\n",
    "# Schema for actual dataframe\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load(file_path)\n",
    "print(aa_dfw_df.printSchema())\n",
    "\n",
    "# Schema for modified dataframe\n",
    "print(aa_dfw_df.select('Date (MM/DD/YYYY)','Destination Airport').printSchema())\n",
    "\n",
    "# Rename the Date (MM/DD/YYYY) -> Date to use it as  df.Date\n",
    "print(aa_dfw_df.withColumn('Date',aa_dfw_df['Date (MM/DD/YYYY)']).drop(aa_dfw_df['Date (MM/DD/YYYY)']).printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c47f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
