{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082afcc1",
   "metadata": {},
   "source": [
    "## Lab Prerequisit\n",
    "1) MySql jar is added in classpath of Spark\n",
    "> You can do it by having a symbolic link in SPARK_HOME/jars to MySql jar\n",
    ">> `~$` ln -s /usr/share/java/mysql-connector-java-5.1.45.jar /home/talentum/spark/jars/mysql-connector-java.jar\n",
    "Ref - https://www.cyberciti.biz/faq/creating-soft-link-or-symbolic-link/\n",
    "\n",
    "2) cd to ~/test-jupyter/test/ on your apache sandbox\n",
    "\n",
    "3) test`$` cp salaries.txt /tmp\n",
    "\n",
    "4) test`$` mysql -u bigdata -p\n",
    "password Bigdata@123\n",
    "\n",
    "5) mysql>CREATE DATABASE test;\n",
    "\n",
    "6) Mysql>use test;\n",
    "\n",
    "7) Mysql>drop table if exists salaries;\n",
    "\n",
    "8) Mysql>create table salaries (\n",
    "gender varchar(1),\n",
    "age int,\n",
    "salary double,\n",
    "zipcode int);\n",
    "\n",
    "9) Mysql>load data local infile '/tmp/salaries.txt' into table salaries fields terminated by ',';\n",
    "\n",
    "10) Mysql>alter table salaries add column `id` int(10) unsigned primary KEY AUTO_INCREMENT;\n",
    "\n",
    "11) Quit MySql\n",
    "> Mysql>quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77cef3f",
   "metadata": {},
   "source": [
    "```shell\n",
    "talentum@talentum-virtual-machine:~/test-jupyter/Spark-MySql$ ls -lh ~/spark/jars/\n",
    "total 231M\n",
    ".....\n",
    "......\n",
    ".......\n",
    "lrwxrwxrwx 1 talentum talentum    47 Jan  7 15:38 mysql-connector-java.jar -> /usr/share/java/mysql-connector-java-5.1.45.jar\n",
    ".......\n",
    "......\n",
    ".....\n",
    "talentum@talentum-virtual-machine:~/test-jupyter/Spark-MySql$ ls -lh \n",
    "total 12K\n",
    "-rwxrwx--- 1 talentum talentum  782 Jan  7 15:30 salaries.txt\n",
    "-rwxrwx--- 1 talentum talentum 7.0K Jan  7 15:35 Spark-MySql.ipynb\n",
    "talentum@talentum-virtual-machine:~/test-jupyter/Spark-MySql$ wc -l salaries.txt \n",
    "50 salaries.txt\n",
    "talentum@talentum-virtual-machine:~/test-jupyter/Spark-MySql$ mysql -u bigdata -p\n",
    "Enter password: \n",
    "Welcome to the MySQL monitor.  Commands end with ; or \\g.\n",
    "Your MySQL connection id is 2\n",
    "Server version: 5.7.42-0ubuntu0.18.04.1 (Ubuntu)\n",
    "\n",
    "Copyright (c) 2000, 2023, Oracle and/or its affiliates.\n",
    "\n",
    "Oracle is a registered trademark of Oracle Corporation and/or its\n",
    "affiliates. Other names may be trademarks of their respective\n",
    "owners.\n",
    "\n",
    "Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n",
    "\n",
    "mysql> CREATE DATABASE test;\n",
    "ERROR 1007 (HY000): Can't create database 'test'; database exists\n",
    "mysql> DROP DATABASE test;\n",
    "Query OK, 0 rows affected (0.03 sec)\n",
    "\n",
    "mysql> CREATE DATABASE test;\n",
    "Query OK, 1 row affected (0.00 sec)\n",
    "\n",
    "mysql> use test;\n",
    "Database changed\n",
    "mysql> drop table if exists salaries;\n",
    "Query OK, 0 rows affected, 1 warning (0.00 sec)\n",
    "mysql> create table salaries ( gender varchar(1), age int, salary double, zipcode int);\n",
    "Query OK, 0 rows affected (0.06 sec)\n",
    "\n",
    "mysql> load data local infile '/tmp/salaries.txt' into table salaries fields terminated by ',';\n",
    "Query OK, 50 rows affected (0.02 sec)\n",
    "Records: 50  Deleted: 0  Skipped: 0  Warnings: 0\n",
    "\n",
    "mysql> alter table salaries add column id int(10) unsigned primary KEY AUTO_INCREMENT;\n",
    "Query OK, 0 rows affected (0.21 sec)\n",
    "Records: 0  Duplicates: 0  Warnings: 0\n",
    "\n",
    "mysql> quit;\n",
    "Bye\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db16e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa07e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a96fca",
   "metadata": {},
   "source": [
    "## Pyspark working with MySql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2170bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:mysql://127.0.0.1:3306/test?useSSL=false&allowPublicKeyRetrieval=true\"\n",
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "user = \"bigdata\"\n",
    "password = \"Bigdata@123\"\n",
    "\n",
    "# https://youtu.be/ray3YvnIohM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3acfb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"driver\", driver)\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"user\", user)\\\n",
    "    .option(\"password\", password)\\\n",
    "    .option(\"dbtable\", \"salaries\")\\\n",
    "    .load()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932f7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+-------+---+\n",
      "|gender|age| salary|zipcode| id|\n",
      "+------+---+-------+-------+---+\n",
      "|     F| 66|41000.0|  95103|  1|\n",
      "|     M| 40|76000.0|  95102|  2|\n",
      "|     F| 58|95000.0|  95103|  3|\n",
      "|     F| 68|60000.0|  95105|  4|\n",
      "|     M| 85|14000.0|  95102|  5|\n",
      "+------+---+-------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df =  spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"driver\", driver)\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"user\", user)\\\n",
    "    .option(\"password\", password)\\\n",
    "    .option(\"dbtable\", \"salaries\")\\\n",
    "    .load()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb13843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|gender|max(gender)|max(salary)|\n",
      "+------+-----------+-----------+\n",
      "|     F|          F|    95000.0|\n",
      "|     M|          M|    99000.0|\n",
      "+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('gender').agg({\n",
    "    \"gender\":\"max\",\n",
    "    \"salary\":\"max\"\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67d3d7",
   "metadata": {},
   "source": [
    "# Reading From Database in Parallel\n",
    "\n",
    "When we are reading large table, we would like to read that in parallel. This will dramatically improve read performance. We can pass “numPartitions” option to spark read function which will decide parallelism in reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96337340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"driver\", driver)\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"user\", user)\\\n",
    "    .option(\"password\", password)\\\n",
    "    .option(\"dbtable\", \"salaries\")\\\n",
    "    .option(\"numPartitions\", 10)\\\n",
    "    .load()\n",
    " \n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1cb18",
   "metadata": {},
   "source": [
    "In our case, it will still show as 1 partition only. This is because we do not have enough data to create 10 different partitions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
